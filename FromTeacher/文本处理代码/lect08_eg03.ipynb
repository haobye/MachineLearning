{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本特征及分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'I': True, 'like': True, 'movie': True, 'much': True, '!': True}, 1], [{'That': True, 'good': True, 'movie': True, '.': True}, 1], [{'This': True, 'great': True, 'one': True, '.': True}, 1], [{'That': True, 'really': True, 'bad': True, 'movie': True, '.': True}, 0], [{'This': True, 'terrible': True, 'movie': True, '.': True}, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 简单的例子\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "text1 = 'I like the movie so much!'\n",
    "text2 = 'That is a good movie.'\n",
    "text3 = 'This is a great one.'\n",
    "text4 = 'That is a really bad movie.'\n",
    "text5 = 'This is a terrible movie.'\n",
    "\n",
    "def proc_text(text):\n",
    "    \"\"\"\n",
    "        预处处理文本\n",
    "    \"\"\"\n",
    "    # 分词\n",
    "    raw_words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # 词形归一化\n",
    "    wordnet_lematizer = WordNetLemmatizer()    \n",
    "    words = [wordnet_lematizer.lemmatize(raw_word) for raw_word in raw_words]\n",
    "    \n",
    "    # 去除停用词\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # True 表示该词在文本中，为了使用nltk中的分类器\n",
    "    return {word: True for word in filtered_words}\n",
    "\n",
    "# 构造训练样本\n",
    "train_data = [[proc_text(text1), 1],\n",
    "              [proc_text(text2), 1],\n",
    "              [proc_text(text3), 1],\n",
    "              [proc_text(text4), 0],\n",
    "              [proc_text(text5), 0]]\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "nb_model = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# 测试模型\n",
    "text6 = 'That is a bad one.'\n",
    "print(nb_model.classify(proc_text(text6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 文本相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "text1 = 'I like the movie so much '\n",
    "text2 = 'That is a good movie '\n",
    "text3 = 'This is a great one '\n",
    "text4 = 'That is a really bad movie '\n",
    "text5 = 'This is a terrible movie'\n",
    "\n",
    "text = text1 + text2 + text3 + text4 + text5\n",
    "words = nltk.word_tokenize(text)\n",
    "freq_dist = FreqDist(words)\n",
    "print(freq_dist['That'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 4), ('is', 4), ('a', 4), ('That', 2), ('This', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 取出常用的n=5个单词\n",
    "n = 5\n",
    "\n",
    "# 构造“常用单词列表”\n",
    "most_common_words = freq_dist.most_common(n)\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': 0, 'is': 1, 'a': 2, 'That': 3, 'This': 4}\n"
     ]
    }
   ],
   "source": [
    "def lookup_pos(most_common_words):\n",
    "    \"\"\"\n",
    "        查找常用单词的位置\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    pos = 0\n",
    "    for word in most_common_words:\n",
    "        result[word[0]] = pos\n",
    "        pos += 1\n",
    "    return result\n",
    "\n",
    "# 记录位置\n",
    "std_pos_dict = lookup_pos(most_common_words)\n",
    "print(std_pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 新文本\n",
    "new_text = 'That one is a good movie. This is so good!'\n",
    "\n",
    "# 初始化向量\n",
    "freq_vec = [0] * n\n",
    "\n",
    "# 分词\n",
    "new_words = nltk.word_tokenize(new_text)\n",
    "\n",
    "# 在“常用单词列表”上计算词频\n",
    "for new_word in new_words:\n",
    "    if new_word in list(std_pos_dict.keys()):\n",
    "        freq_vec[std_pos_dict[new_word]] += 1\n",
    "\n",
    "print(freq_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 文本分类及TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 NLTK中的TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That的TF-IDF值为：0.02181644599700369\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "\n",
    "text1 = 'I like the movie so much '\n",
    "text2 = 'That is a good movie '\n",
    "text3 = 'This is a great one '\n",
    "text4 = 'That is a really bad movie '\n",
    "text5 = 'This is a terrible movie'\n",
    "\n",
    "# 构建TextCollection对象\n",
    "tc = TextCollection([text1, text2, text3, \n",
    "                        text4, text5])\n",
    "new_text = 'That one is a good movie. This is so good!'\n",
    "word = 'That'\n",
    "tf_idf_val = tc.tf_idf(word, new_text)\n",
    "print('{}的TF-IDF值为：{}'.format(word, tf_idf_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 sklearn中的TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "feat = vectorizer.fit_transform([text1, text2, text3, text4, text5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'good',\n",
       " 'great',\n",
       " 'is',\n",
       " 'like',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'one',\n",
       " 'really',\n",
       " 'so',\n",
       " 'terrible',\n",
       " 'that',\n",
       " 'the',\n",
       " 'this']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 14)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_array = feat.toarray()\n",
    "feat_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.48127008,\n",
       "        0.27113917,  0.48127008,  0.        ,  0.        ,  0.48127008,\n",
       "        0.        ,  0.        ,  0.48127008,  0.        ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_array[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.67082255,  0.        ,  0.37792972,  0.        ,\n",
       "         0.18896486,  0.        ,  0.33541128,  0.        ,  0.33541128,\n",
       "         0.        ,  0.27060771,  0.        ,  0.27060771]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([new_text]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 中文TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_text1 = ' 非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、妇联内部都没什么火花。团结-分裂-团结的三段式虽然老套但其实也可以利用积攒下来的形象魅力搞出意思，但剧本写得非常肤浅、平面。场面上调度混乱呆板，满屏的铁甲审美疲劳。只有笑点算得上差强人意。'\n",
    "ch_text2 = ' 2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出新，实则俗不可耐；以为场面很high，实则high劲不足。气！上一集的趣味全无，这集的笑点明显刻意到心虚。全片没有任何片段给我有紧张激动的时候，太弱了，跟奥创一样。'\n",
    "ch_text3 = ' 《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于……跟绿巨人表白了，黑寡妇用实际行动告诉了我们什么叫忠贞不二；而且为了治疗不孕不育连作战武器都变成了两支验孕棒(坚决相信快银没有死，后面还得回来)'\n",
    "ch_text4 = ' 虽然从头打到尾，但是真的很无聊啊。'\n",
    "ch_text5 = ' 剧情不如第一集好玩了，全靠密集笑点在提神。僧多粥少的直接后果就是每部寡姐都要换着队友谈恋爱，这特么比打斗还辛苦啊，真心求放过～～～（结尾彩蛋还以为是洛基呢，结果我呸！）'\n",
    "\n",
    "ch_texts = [ch_text1, ch_text2, ch_text3, ch_text4, ch_text5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "corpus = []\n",
    "for ch_text in ch_texts:\n",
    "    corpus.append(' '.join(jieba.cut(ch_text, cut_all=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 或者\n",
    "corpus = [' '.join(jieba.cut(ch_text, cut_all=False)) for ch_text in ch_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  非常 失望 ， 剧本 完全 敷衍了事 ， 主线 剧情 没 突破 大家 可以 理解 ， 可 所有 的 人物 都 缺乏 动机 ， 正邪 之间 、 妇联 内部 都 没什么 火花 。 团结 - 分裂 - 团结 的 三段式 虽然 老套 但 其实 也 可以 利用 积攒 下来 的 形象 魅力 搞 出 意思 ， 但 剧本 写得 非常 肤浅 、 平面 。 场面 上 调度 混乱 呆板 ， 满屏 的 铁甲 审美疲劳 。 只有 笑 点算 得 上 差强人意 。',\n",
       " '  2015 年度 最 失望 作品 。 以为 面面俱到 ， 实则 画蛇添足 ； 以为 主题深刻 ， 实则 老调重弹 ； 以为 推陈出新 ， 实则 俗不可耐 ； 以为 场面 很 high ， 实则 high 劲 不足 。 气 ！ 上 一集 的 趣味 全无 ， 这集 的 笑 点 明显 刻意 到 心虚 。 全片 没有 任何 片段 给 我 有 紧张 激动 的 时候 ， 太弱 了 ， 跟 奥创 一样 。',\n",
       " '  《 铁人 2 》 中 勾引 钢铁 侠 ， 《 妇联 1 》 中 勾引 鹰眼 ， 《 美队 2 》 中 勾引 美国 队长 ， 在 《 妇联 2 》 中 终于 … … 跟 绿巨人 表白 了 ， 黑寡妇 用 实际行动 告诉 了 我们 什么 叫 忠贞不二 ； 而且 为了 治疗 不孕 不育 连 作战 武器 都 变成 了 两支 验孕 棒 ( 坚决 相信 快银 没有 死 ， 后面 还 得 回来 )',\n",
       " '  虽然 从头 打到 尾 ， 但是 真的 很 无聊 啊 。',\n",
       " '  剧情 不如 第一集 好玩 了 ， 全靠 密集 笑点 在 提神 。 僧多粥少 的 直接 后果 就是 每部 寡姐 都 要 换 着 队友 谈恋爱 ， 这特 么 比 打斗 还 辛苦 啊 ， 真心 求 放过 ～ ～ ～ （ 结尾 彩蛋 还 以为 是 洛基 呢 ， 结果 我 呸 ！ ）']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_vectorizer = TfidfVectorizer()\n",
    "ch_feats = ch_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.13332791,\n",
       "        0.13332791,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.13332791,  0.        ,  0.13332791,\n",
       "        0.13332791,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.13332791,  0.13332791,\n",
       "        0.13332791,  0.13332791,  0.13332791,  0.        ,  0.10756811,\n",
       "        0.26665581,  0.13332791,  0.        ,  0.        ,  0.13332791,\n",
       "        0.26665581,  0.        ,  0.        ,  0.13332791,  0.        ,\n",
       "        0.        ,  0.26665581,  0.10756811,  0.        ,  0.13332791,\n",
       "        0.        ,  0.10756811,  0.        ,  0.        ,  0.10756811,\n",
       "        0.13332791,  0.        ,  0.        ,  0.13332791,  0.        ,\n",
       "        0.        ,  0.        ,  0.13332791,  0.13332791,  0.        ,\n",
       "        0.13332791,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.13332791,  0.        ,  0.13332791,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.13332791,  0.        ,\n",
       "        0.        ,  0.        ,  0.13332791,  0.        ,  0.        ,\n",
       "        0.13332791,  0.        ,  0.        ,  0.        ,  0.13332791,\n",
       "        0.13332791,  0.        ,  0.13332791,  0.13332791,  0.        ,\n",
       "        0.13332791,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.13332791,  0.13332791,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.13332791,  0.        ,  0.        ,  0.13332791,  0.        ,\n",
       "        0.        ,  0.13332791,  0.10756811,  0.        ,  0.13332791,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.13332791,  0.        ,  0.        ,\n",
       "        0.26665581,  0.        ,  0.        ,  0.13332791,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_feats.toarray()[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
