{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown # 需要下载brown语料库\n",
    "# 引用布朗大学的语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# 查看语料库包含的类别\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有57340个句子\n",
      "共有1161192个单词\n"
     ]
    }
   ],
   "source": [
    "# 查看brown语料库\n",
    "print('共有{}个句子'.format(len(brown.sents())))\n",
    "print('共有{}个单词'.format(len(brown.words())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 词频统计\n",
    "from nltk import FreqDist\n",
    "dist = FreqDist(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非重复单词个数： 56057\n",
      "前10个单词： [\"Yorker's\", 'carted', 'Paulah', 'Wires', 'grated', 'oops', 'friezes', 'save', 'wiggle', 'furnishes']\n",
      "the出现的个数： 62713\n"
     ]
    }
   ],
   "source": [
    "print('非重复单词个数：', len(dist))\n",
    "print('前10个单词：', list(dist.keys())[:10])\n",
    "print('the出现的个数：', dist['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找出长度大于5，且出现次数大于500的单词\n",
    "freq_words = [w for w in dist.keys() if len(w) > 5 and dist[w] > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['should',\n",
       " 'around',\n",
       " 'another',\n",
       " 'against',\n",
       " 'people',\n",
       " 'between',\n",
       " 'American',\n",
       " 'without',\n",
       " 'before',\n",
       " 'through',\n",
       " 'because',\n",
       " 'himself',\n",
       " 'thought',\n",
       " 'little']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NLTK英文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = \"Python is a widely used high-level programming language for general-purpose programming.\"\n",
    "tokens = nltk.word_tokenize(sentence) # 需要下载punkt分词模型\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = 'Python is a widely used high-level programming language for general-purpose programming, created by Guido van Rossum and first released in 1991. An interpreted language, Python has a design philosophy that emphasizes code readability (notably using whitespace indentation to delimit code blocks rather than curly brackets or keywords), and a syntax that allows programmers to express concepts in fewer lines of code than might be used in languages such as C++ or Java.[23][24] The language provides constructs intended to enable writing clear programs on both a small and large scale.'\n",
    "sentences = nltk.sent_tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 中文结巴分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 安装 pip install jieba\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"欢迎来到小象学院\", cut_all=True)\n",
    "print(\"全模式: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"欢迎来到小象学院\", cut_all=False)\n",
    "print(\"精确模式: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 词形归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 词干提取(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('looked'))\n",
    "print(porter_stemmer.stem('went'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input1 = 'List listed lists listing listings'\n",
    "words1 = input1.lower().split(' ')\n",
    "[porter_stemmer.stem(w) for w in words1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SnowballStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "print(snowball_stemmer.stem('looked'))\n",
    "print(snowball_stemmer.stem('looking'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LancasterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print(lancaster_stemmer.stem('looked'))\n",
    "print(lancaster_stemmer.stem('looking'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 词形归并(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer # 需要下载wordnet语料库\n",
    "\n",
    "wordnet_lematizer = WordNetLemmatizer()\n",
    "print(wordnet_lematizer.lemmatize('cats'))\n",
    "print(wordnet_lematizer.lemmatize('boxes'))\n",
    "print(wordnet_lematizer.lemmatize('are'))\n",
    "print(wordnet_lematizer.lemmatize('went'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指明词性可以更准确地进行lemma\n",
    "# lemmatize 默认为名词\n",
    "print(wordnet_lematizer.lemmatize('are', pos='v'))\n",
    "print(wordnet_lematizer.lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 词性标注 (Part-Of-Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "words = nltk.word_tokenize('Python is a widely used programming language.')\n",
    "#print(words)\n",
    "print(nltk.pos_tag(words)) # 需要下载 averaged_perceptron_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # 需要下载stopwords\n",
    "\n",
    "filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "print('原始词：', words)\n",
    "print('去除停用词后：', filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 典型的文本预处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 原始文本\n",
    "raw_text = 'Life is like a box of chocolates. You never know what you\\'re gonna get.'\n",
    "\n",
    "# 分词\n",
    "raw_words = nltk.word_tokenize(raw_text)\n",
    "\n",
    "# 词形归一化\n",
    "wordnet_lematizer = WordNetLemmatizer()\n",
    "words = [wordnet_lematizer.lemmatize(raw_word) for raw_word in raw_words]\n",
    "\n",
    "# 去除停用词\n",
    "filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "print('原始文本：', raw_text)\n",
    "print('预处理结果：', filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
