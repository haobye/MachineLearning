{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数说明:接收一个大字符串并将其解析为字符串列表\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    无\n",
    "\"\"\"\n",
    "def textParse(bigString):                                                   #将字符串转换为字符列表\n",
    "    # * 会匹配0个或多个规则，split会将字符串分割成单个字符【python3.5+】; 这里使用\\W 或者\\W+ 都可以将字符数字串分割开，\n",
    "    #产生的空字符将会在后面的列表推导式中过滤掉\n",
    "    listOfTokens = re.split(r'\\W+', bigString)                              #将特殊符号作为切分标志进行字符串切分，即非字母、非数字\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]            #除了单个字母，例如大写的I，其它单词变成小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataSet():\n",
    "    docList = []; classList = []\n",
    "    for i in range(1, 26):                                                  #遍历25个txt文件\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, 'r').read())     #读取每个垃圾邮件，并字符串转换成字符串列表\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)                                                 #标记垃圾邮件，1表示垃圾文件\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, 'r').read())      #读取每个非垃圾邮件，并字符串转换成字符串列表\n",
    "        docList.append(wordList)\n",
    "        classList.append(0)                 \n",
    "    return docList,classList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docList,classList = get_dataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2、构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：将切分的样本词条整理成词汇表（不重复）\n",
    "参数说明：\n",
    "dataSet：切分好的样本词条\n",
    "返回：\n",
    "vocabList：不重复的词汇表\n",
    "\"\"\"\n",
    "def createVocablist(dataSet):\n",
    "    vocabSet = set()               #创建一个空的集合，set可用于去重\n",
    "    for doc in dataSet:\n",
    "        vocabSet = vocabSet | set(doc)     #取并集\n",
    "        vocablist = list(vocabSet)\n",
    "    return vocablist   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0\n",
    "参数说明：\n",
    "vocabList：词汇表\n",
    "inputSet：切分好的词条列表中的一条  （dataSet中的一条数据）\n",
    "返回：\n",
    "returnVec：文档向量,词集模型\n",
    "\"\"\"\n",
    "def setOfWords2Vec(vocablist, inputSet):\n",
    "    returnVec = [0] * len(vocablist)                  #创建一个其中所含元素都为0的向量\n",
    "    for word in inputSet:                             #遍历每个词条\n",
    "        if word in vocablist:                         #如果词条存在于词汇表中，则变为1\n",
    "            returnVec[vocablist.index(word)] = 1\n",
    "        else:\n",
    "            print(\" {} is not in my Vocabulary!\".format(word) )\n",
    "    return returnVec                                  #返回文档向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 生成训练集向量列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：生成训练集向量列表\n",
    "参数说明：\n",
    "dataSet：切分好的样本词条\n",
    "返回：\n",
    "trainMat：所有的词条向量组成的列表\n",
    "\"\"\"\n",
    "def get_trainMat(dataSet):\n",
    "    trainMat = []                                         #初始化向量列表\n",
    "    vocablist = createVocablist(dataSet)                  #生成词汇表\n",
    "    for inputSet in dataSet:                              #遍历样本词条中的每一条样本\n",
    "        returnVec=setOfWords2Vec(vocablist, inputSet)     #将当前词条向量化\n",
    "        trainMat.append(returnVec)                        #追加到向量列表中\n",
    "    return trainMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：朴素贝叶斯分类器训练函数\n",
    "参数说明：\n",
    "trainMat：训练文档矩阵\n",
    "classVec：训练类别标签向量  classVec = [0,1,0,1,0,1]\n",
    "返回：\n",
    "p0V：非侮辱类的条件概率数组\n",
    "p1V：侮辱类的条件概率数组\n",
    "pAb：文档属于侮辱类的概率\n",
    "\"\"\"\n",
    "def trainNB(trainMat,classVec):\n",
    "    n = len(trainMat)                     #计算训练的文档数目\n",
    "    m = len(trainMat[0])                  #计算每篇文档的词条数\n",
    "    pAb = sum(classVec)/n                 #文档属于侮辱类的概率\n",
    "    p0Num = np.ones(m)                    #词条出现数初始化为1\n",
    "    p1Num = np.ones(m)                    #词条出现数初始化为1\n",
    "    p0Denom = 2                           #分母初始化为2\n",
    "    p1Denom = 2                           #分母初始化为2\n",
    "    for i in range(n):                    #遍历每一个文档\n",
    "        if classVec[i] == 1:              #统计属于侮辱类的条件概率所需的数据\n",
    "            p1Num += trainMat[i]\n",
    "            p1Denom += sum(trainMat[i])\n",
    "        else:                             #统计属于非侮辱类的条件概率所需的数据\n",
    "            p0Num += trainMat[i]\n",
    "            p0Denom += sum(trainMat[i])\n",
    "    p1V = np.log(p1Num/p1Denom)                                      \n",
    "    p0V = np.log(p0Num/p0Denom)        \n",
    "    return p0V,p1V,pAb                    #返回属于非侮辱类,侮辱类和文档属于侮辱类的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\"\"\"\n",
    "函数功能：朴素贝叶斯分类器分类函数\n",
    "参数说明：\n",
    "vec2Classify：待分类的词条数组,已经向量化的测试样本\n",
    "p0V：非侮辱类的条件概率数组\n",
    "p1V：侮辱类的条件概率数组\n",
    "pAb：文档属于侮辱类的概率\n",
    "返回：\n",
    "0：属于非侮辱类\n",
    "1：属于侮辱类\n",
    "\"\"\"\n",
    "def classifyNB(vec2Classify, p0V, p1V, pAb):\n",
    "    p1 = sum(vec2Classify * p1V) + np.log(pAb)       #对应元素相乘\n",
    "    p0 = sum(vec2Classify * p0V) + np.log(1- pAb)    #对应元素相乘\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：产生训练集和测试集的索引值\n",
    "\n",
    "\"\"\"\n",
    "def createtest():\n",
    "    trainingSet = list(range(len(classList))); testSet = []                 #创建存储训练集的索引值的列表和测试集的索引值的列表  \n",
    "    for i in range(10):                                                     #从50个邮件中，随机挑选出40个作为训练集,10个做测试集\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))                #随机选取索索引值\n",
    "        testSet.append(trainingSet[randIndex])                              #添加测试集的索引值\n",
    "        del(trainingSet[randIndex])                                         #在训练集列表中删除添加到测试集的索引值   \n",
    "    return trainingSet,testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSet,testSet = createtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codeine', '15mg', 'for', '203', 'visa', 'only', 'codeine', 'methylmorphine', 'narcotic', 'opioid', 'pain', 'reliever', 'have', '15mg', '30mg', 'pills', '15mg', 'for', '203', '15mg', 'for', '385', '15mg', 'for', '562', 'visa', 'only']\n",
      "['peter', 'with', 'jose', 'out', 'town', 'you', 'want', 'meet', 'once', 'while', 'keep', 'things', 'going', 'and', 'some', 'interesting', 'stuff', 'let', 'know', 'eugene']\n",
      "['hydrocodone', 'vicodin', 'brand', 'watson', 'vicodin', '750', '195', '120', '570', 'brand', 'watson', '750', '195', '120', '570', 'brand', 'watson', '325', '199', '120', '588', 'noprescription', 'required', 'free', 'express', 'fedex', 'days', 'delivery', 'for', 'over', '200', 'order', 'major', 'credit', 'cards', 'check']\n",
      "['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe', 'the', 'proven', 'naturalpenisenhancement', 'that', 'works', '100', 'moneyback', 'guaranteeed']\n",
      "['percocet', '625', 'withoutprescription', 'tabs', '225', 'percocet', 'narcotic', 'analgesic', 'used', 'treat', 'moderate', 'moderately', 'severepain', 'top', 'quality', 'express', 'shipping', '100', 'safe', 'discreet', 'private', 'buy', 'cheap', 'percocet', 'online']\n",
      "['codeine', '15mg', 'for', '203', 'visa', 'only', 'codeine', 'methylmorphine', 'narcotic', 'opioid', 'pain', 'reliever', 'have', '15mg', '30mg', 'pills', '15mg', 'for', '203', '15mg', 'for', '385', '15mg', 'for', '562', 'visa', 'only']\n",
      "['there', 'was', 'guy', 'the', 'gas', 'station', 'who', 'told', 'that', 'knew', 'mandarin', 'and', 'python', 'could', 'get', 'job', 'with', 'the', 'fbi']\n",
      "['hello', 'since', 'you', 'are', 'owner', 'least', 'one', 'google', 'groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'files', 'are', 'writing', 'inform', 'you', 'that', 'will', 'longer', 'supporting', 'these', 'features', 'starting', 'february', '2011', 'made', 'this', 'decision', 'that', 'can', 'focus', 'improving', 'the', 'core', 'functionalities', 'google', 'groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'instead', 'these', 'features', 'encourage', 'you', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'google', 'docs', 'and', 'google', 'sites', 'for', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'google', 'sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'answer', '174623', 'with', 'the', 'members', 'your', 'group', 'you', 'can', 'also', 'store', 'your', 'files', 'the', 'site', 'attaching', 'files', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'answer', '90563', 'the', 'site', 'you抮e', 'just', 'looking', 'for', 'place', 'upload', 'your', 'files', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'suggest', 'you', 'try', 'google', 'docs', 'you', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'answer', '66343', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'answer', '86152', 'assigning', 'either', 'edit', 'download', 'only', 'access', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'update', 'you', 'about', 'important', 'changes', 'google', 'groups']\n",
      "['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe']\n",
      "['this', 'mail', 'was', 'sent', 'from', 'notification', 'only', 'address', 'that', 'cannot', 'accept', 'incoming', 'mail', 'please', 'not', 'reply', 'this', 'message', 'thank', 'you', 'for', 'your', 'online', 'reservation', 'the', 'store', 'you', 'selected', 'has', 'located', 'the', 'item', 'you', 'requested', 'and', 'has', 'placed', 'hold', 'your', 'name', 'please', 'note', 'that', 'all', 'items', 'are', 'held', 'for', 'day', 'please', 'note', 'store', 'prices', 'may', 'differ', 'from', 'those', 'online', 'you', 'have', 'questions', 'need', 'assistance', 'with', 'your', 'reservation', 'please', 'contact', 'the', 'store', 'the', 'phone', 'number', 'listed', 'below', 'you', 'can', 'also', 'access', 'store', 'information', 'such', 'store', 'hours', 'and', 'location', 'the', 'web', 'http', 'www', 'borders', 'com', 'online', 'store', 'storedetailview_98']\n",
      "['bargains', 'here', 'buy', 'phentermin', 'buy', 'genuine', 'phentermin', 'low', 'cost', 'visa', 'accepted', '130', '219', '292', '120', '366', '180', '513']\n",
      "['peter', 'these', 'are', 'the', 'only', 'good', 'scenic', 'ones', 'and', 'too', 'bad', 'there', 'was', 'girl', 'back', 'one', 'them', 'just', 'try', 'enjoy', 'the', 'blue', 'sky']\n",
      "['ordercializviagra', 'online', 'save', '0nline', 'pharmacy', 'noprescription', 'required', 'buy', 'canadian', 'drugs', 'wholesale', 'prices', 'and', 'save', 'fda', 'approved', 'drugs', 'superb', 'quality', 'drugs', 'only', 'accept', 'all', 'major', 'credit', 'cards']\n",
      "['ryan', 'whybrew', 'commented', 'your', 'status', 'ryan', 'wrote', 'turd', 'ferguson', 'butt', 'horn']\n",
      "['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe', 'the', 'proven', 'naturalpenisenhancement', 'that', 'works', '100', 'moneyback', 'guaranteeed']\n",
      "['arvind', 'thirumalai', 'commented', 'your', 'status', 'arvind', 'wrote', 'you', 'know', 'reply', 'this', 'email', 'comment', 'this', 'status']\n",
      "['buy', 'ambiem', 'zolpidem', '5mg', '10mg', 'pill', 'pills', '129', 'pills', '199', '180', 'pills', '430', 'pills', '138', '120', 'pills', '322']\n",
      "['thanks', 'peter', 'definitely', 'check', 'this', 'how', 'your', 'book', 'going', 'heard', 'chapter', 'came', 'and', 'was', 'good', 'shape', 'hope', 'you', 'are', 'doing', 'well', 'cheers', 'troy']\n",
      "['ordercializviagra', 'online', 'save', '0nline', 'pharmacy', 'noprescription', 'required', 'buy', 'canadian', 'drugs', 'wholesale', 'prices', 'and', 'save', 'fda', 'approved', 'drugs', 'superb', 'quality', 'drugs', 'only', 'accept', 'all', 'major', 'credit', 'cards', 'order', 'today', 'from']\n",
      "['jay', 'stepp', 'commented', 'your', 'status', 'jay', 'wrote', 'the', 'reply', 'this', 'email', 'comment', 'this', 'status', 'see', 'the', 'comment', 'thread', 'follow', 'the', 'link', 'below']\n",
      "['buyviagra', '25mg', '50mg', '100mg', 'brandviagra', 'femaleviagra', 'from', 'per', 'pill', 'viagranoprescription', 'needed', 'from', 'certified', 'canadian', 'pharmacy', 'buy', 'here', 'accept', 'visa', 'amex', 'check', 'worldwide', 'delivery']\n",
      "['linkedin', 'kerry', 'haloney', 'requested', 'add', 'you', 'connection', 'linkedin', 'peter', 'like', 'add', 'you', 'professional', 'network', 'linkedin', 'kerry', 'haloney']\n",
      "['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe']\n",
      "['peter', 'the', 'hotels', 'are', 'the', 'ones', 'that', 'rent', 'out', 'the', 'tent', 'they', 'are', 'all', 'lined', 'the', 'hotel', 'grounds', 'much', 'for', 'being', 'one', 'with', 'nature', 'more', 'like', 'being', 'one', 'with', 'couple', 'dozen', 'tour', 'groups', 'and', 'nature', 'have', 'about', '100m', 'pictures', 'from', 'that', 'trip', 'can', 'through', 'them', 'and', 'get', 'you', 'jpgs', 'favorite', 'scenic', 'pictures', 'where', 'are', 'you', 'and', 'jocelyn', 'now', 'new', 'york', 'will', 'you', 'come', 'tokyo', 'for', 'chinese', 'new', 'year', 'perhaps', 'see', 'the', 'two', 'you', 'then', 'will', 'thailand', 'for', 'winter', 'holiday', 'see', 'mom', 'take', 'care']\n",
      "['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe']\n",
      "['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don抰', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
      "['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']\n",
      "['codeine', 'the', 'most', 'competitive', 'price', 'net', 'codeine', 'wilson', '30mg', '156', 'codeine', 'wilson', '30mg', '291', 'freeviagra', 'pills', 'codeine', 'wilson', '30mg', '396', 'freeviagra', 'pills', 'codeine', 'wilson', '30mg', '120', '492', 'freeviagra', 'pills']\n",
      "['peter', 'sure', 'thing', 'sounds', 'good', 'let', 'know', 'what', 'time', 'would', 'good', 'for', 'you', 'will', 'come', 'prepared', 'with', 'some', 'ideas', 'and', 'can', 'from', 'there', 'regards', 'vivek']\n",
      "['linkedin', 'julius', 'requested', 'add', 'you', 'connection', 'linkedin', 'peter', 'looking', 'forward', 'the', 'book', 'accept', 'view', 'invitation', 'from', 'julius']\n",
      "['get', 'off', 'online', 'watchesstore', 'discount', 'watches', 'for', 'all', 'famous', 'brands', 'watches', 'arolexbvlgari', 'dior', 'hermes', 'oris', 'cartier', 'and', 'more', 'brands', 'louis', 'vuitton', 'bags', 'wallets', 'gucci', 'bags', 'tiffany', 'jewerly', 'enjoy', 'full', 'year', 'warranty', 'shipment', 'via', 'reputable', 'courier', 'fedex', 'ups', 'dhl', 'and', 'ems', 'speedpost', 'you', 'will', '100', 'recieve', 'your', 'order']\n",
      "['thought', 'about', 'this', 'and', 'think', 'possible', 'should', 'get', 'another', 'lunch', 'have', 'car', 'now', 'and', 'could', 'come', 'pick', 'you', 'this', 'time', 'does', 'this', 'wednesday', 'work', 'can', 'have', 'signed', 'copy', 'you', 'book']\n",
      "['get', 'off', 'online', 'watchesstore', 'discount', 'watches', 'for', 'all', 'famous', 'brands', 'watches', 'arolexbvlgari', 'dior', 'hermes', 'oris', 'cartier', 'and', 'more', 'brands', 'louis', 'vuitton', 'bags', 'wallets', 'gucci', 'bags', 'tiffany', 'jewerly', 'enjoy', 'full', 'year', 'warranty', 'shipment', 'via', 'reputable', 'courier', 'fedex', 'ups', 'dhl', 'and', 'ems', 'speedpost', 'you', 'will', '100', 'recieve', 'your', 'order']\n",
      "['hommies', 'just', 'got', 'phone', 'call', 'from', 'the', 'roofer', 'they', 'will', 'come', 'and', 'spaying', 'the', 'foaming', 'today', 'will', 'dusty', 'pls', 'close', 'all', 'the', 'doors', 'and', 'windows', 'could', 'you', 'help', 'close', 'bathroom', 'window', 'cat', 'window', 'and', 'the', 'sliding', 'door', 'behind', 'the', 'don', 'know', 'how', 'can', 'those', 'cats', 'survive', 'sorry', 'for', 'any', 'inconvenience']\n",
      "['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe']\n",
      "['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe']\n",
      "['will', 'there', 'the', 'latest']\n",
      "['experience', 'with', 'biggerpenis', 'today', 'grow', 'inches', 'more', 'the', 'safest', 'most', 'effective', 'methods', 'of_penisen1argement', 'save', 'your', 'time', 'and', 'money', 'bettererections', 'with', 'effective', 'ma1eenhancement', 'products', 'ma1eenhancement', 'supplement', 'trusted', 'millions', 'buy', 'today']\n"
     ]
    }
   ],
   "source": [
    "for docIndex in trainingSet:\n",
    "    print(docList[docIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能：朴素贝叶斯测试函数\n",
    "参数说明：\n",
    "testVec：测试样本\n",
    "返回：测试样本的类别\n",
    "\"\"\"\n",
    "def testingNB():\n",
    "    docList,classList = get_dataSet()                       #创建实验样本\n",
    "    vocabList = createVocablist(docList)                                    #创建词汇表，不重复 \n",
    "    trainMat = []; trainClasses = []                                        #创建训练集矩阵和训练集类别标签系向量             \n",
    "    for docIndex in trainingSet:                                            #遍历训练集\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))       #将生成的词集模型添加到训练矩阵中\n",
    "        trainClasses.append(classList[docIndex])                            #将类别添加到训练集类别标签系向量中\n",
    "    p0V,p1V,pAb = trainNB(np.array(trainMat), np.array(trainClasses))               #训练朴素贝叶斯分类器\n",
    "    errorCount = 0                                                          #错误分类计数\n",
    "    for docIndex in testSet:                                                #遍历测试集\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])           #测试集的词集模型\n",
    "        if classifyNB(np.array(wordVector), p0V,p1V,pAb) != classList[docIndex]:    #如果分类错误\n",
    "            errorCount += 1                                                 #错误计数加1\n",
    "            print(\"分类错误的测试集：\",docList[docIndex])\n",
    "            print(docIndex)\n",
    "    print('错误率：%.2f%%' % (float(errorCount) / len(testSet) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类错误的测试集： ['oem', 'adobe', 'microsoft', 'softwares', 'fast', 'order', 'and', 'download', 'microsoft', 'office', 'professional', 'plus', '2007', '2010', '129', 'microsoft', 'windows', 'ultimate', '119', 'adobe', 'photoshop', 'cs5', 'extended', 'adobe', 'acrobat', 'pro', 'extended', 'windows', 'professional', 'thousand', 'more', 'titles']\n",
      "10\n",
      "错误率：10.00%\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
